{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with processes/threads \n",
    "------------------------------------------------------\n",
    "\n",
    "<br>\n",
    "\n",
    "## Table of Content <a id=\"toc\"></a>\n",
    "\n",
    "\n",
    "1. [Multiprocessing (and refactoring)](#8)\n",
    "2. [numba and parallelization](#9)\n",
    "\n",
    "    2.1. [automatic parallelization](#2.1)\n",
    "    \n",
    "    2.2. [explicit parallelization (prange)](#2.2)\n",
    "    \n",
    "    2.3. [controling the number of threads used](#2.3)\n",
    "\n",
    "Annex A - [Parallelization of pairwise distance computation with multiprocess](#annexa)\n",
    "\n",
    "Annex B - [Parallelization of pairwise distance computation with numba](#annexb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Multiprocessing (and refactoring) <a id='8'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can take advantage of multiple cores using the `multiprocessing` module. \n",
    "\n",
    "In this approach, separate __processes__ are used, __not threads__. \n",
    "\n",
    "The use of threads is generally blocked by Python because of the \"*Global Interpreter Lock*\". This was a necessary design feature as a trade-off for the enormous flexibility in memory management that Python makes possible. This means that there is no shared memory when using multiprocessing, and thus the individual tasks must be independent.\n",
    "\n",
    "`multiprocessing` generally works well with lists, where one maps a function to each element of the list and these operations are computed as separated processes on separate cores per element of the list. \n",
    "\n",
    "Indeed, any kind of parralelization technique is really only worth it if the task you want to do is actually *parallelizable*. It is sometimes hard to judge what is and is not easily paralellizable, and can often require that you refactor your code quite a bit.\n",
    "\n",
    "A rule of thumb for parallelization is that the task can be divided up is subtasks which : \n",
    "1. **do not depend on each others results**\n",
    "2. are very similar\n",
    "3. use independent part of the data\n",
    "\n",
    "Point 1. is the most important, the others are helpful but not entirely necessary.\n",
    "\n",
    "Consider our function to compute an integral from the previous lesson:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6467999999999999\n",
      "149 ms ± 2.26 ms per loop (mean ± std. dev. of 7 runs, 3 loops each)\n"
     ]
    }
   ],
   "source": [
    "## pure python \n",
    "def f_native(x):\n",
    "    return x ** 2 - x\n",
    "\n",
    "\n",
    "def integrate_f_native(a, b, N):\n",
    "    s = 0\n",
    "    dx = (b - a) / N\n",
    "    for i in range(N):\n",
    "        s += f_native(a + i * dx)\n",
    "    return s * dx\n",
    "\n",
    "print( integrate_f_native(0,2,100) )\n",
    "%timeit -n 3 -r 7 _=integrate_f_native(0,2,1000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideally, we would like to reduce this something that looks like:\n",
    "\n",
    "```python\n",
    "\n",
    "for i in range(len(data)):\n",
    "    result[i] = function(data[i])\n",
    "\n",
    "```\n",
    "equivalent to:\n",
    "```python\n",
    "map(function,data)\n",
    "```\n",
    "\n",
    "So, we apply a `function` to each element (`data[i]`) of `data`.\n",
    "\n",
    "So the game is to re-write it slightly so it fits this template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.666664666668016\n"
     ]
    }
   ],
   "source": [
    "#let's work outside the function and focus on the main loop:\n",
    "a=0\n",
    "b=2\n",
    "N=1000000\n",
    "dx = (b - a) / N\n",
    "\n",
    "data = [ i for i in range(N)]\n",
    "\n",
    "def f2(i):\n",
    "    x = a+i*dx\n",
    "    return x ** 2 - x\n",
    "\n",
    "result = map( f2 , data )\n",
    "## equivalent to\n",
    "# for i in range(N):\n",
    "#     result[i] = f_native(data[i])\n",
    "\n",
    "\n",
    "final_result = sum(result) *dx\n",
    "print(final_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, everything is ready for us to use `multiprocessing`.\n",
    "\n",
    "The simplest usage is to open up a pool of processes using the `with` keyword:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.666664666668016\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing as mp\n",
    "\n",
    "with mp.Pool(2) as pool :\n",
    "    \n",
    "    result2 = pool.map(f2, data)\n",
    "    \n",
    "final_result2 = sum(result2) *dx\n",
    "print(final_result2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok so we get the same result when splitting the task on 2 processes, but does it perform faster?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146 ms ± 4.2 ms per loop (mean ± std. dev. of 7 runs, 3 loops each)\n",
      "196 ms ± 7.74 ms per loop (mean ± std. dev. of 7 runs, 3 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 3 -r 7  list( map(f2, data) )\n",
    "with mp.Pool(2) as pool :\n",
    "    %timeit -n 3 -r 7  pool.map(f2, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mhmm, what? the multiprocessing version is even slower...\n",
    "\n",
    "This is due to the fact that opening, closing, and communicating data to and from processes are costly operation.\n",
    "In other words, the **overhead is great with multiprocessing**, and it tends to work better with a few long tasks than with a lot of very small ones (NB: eahc paralelization techniques have different overhead and react differently to this).\n",
    "\n",
    "For instance , let's try with a few, long tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61.8 ms ± 4.49 ms per loop (mean ± std. dev. of 3 runs, 3 loops each)\n"
     ]
    }
   ],
   "source": [
    "# our \"long\" task will be the integrate_f_native function between 0 and i, with 4millions points\n",
    "# which takes around 0.06 second\n",
    "%timeit -n 3 -r 3 integrate_f_native(0,1,4*10**5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.88 s, sys: 0 ns, total: 5.88 s\n",
      "Wall time: 5.88 s\n"
     ]
    }
   ],
   "source": [
    "from time import sleep\n",
    "\n",
    "## around 0.06sec per task\n",
    "def task(i):\n",
    "    return integrate_f_native(0,i,4*10**5)\n",
    "\n",
    "## 100 tasks to perform\n",
    "data = list(range(1,101))\n",
    "\n",
    "\n",
    "#serial execution: ~6seconds\n",
    "%time _ = list(map(task,data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.25 ms, sys: 158 µs, total: 1.41 ms\n",
      "Wall time: 3.19 s\n"
     ]
    }
   ],
   "source": [
    "with mp.Pool(2) as pool :\n",
    "    %time  pool.map(task, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With such long tasks, the overhead is lower than the gained time.\n",
    "\n",
    "Indeed, on the basis of 0.06 seconds per task, we would expect 100 tasks on 2 processes to take ~3seconds, so we have ~0.45 seconds of overhead here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "CPU times: user 1.73 ms, sys: 218 µs, total: 1.95 ms\n",
      "Wall time: 6.13 s\n",
      "2\n",
      "CPU times: user 29.3 ms, sys: 649 µs, total: 29.9 ms\n",
      "Wall time: 3.33 s\n",
      "4\n",
      "CPU times: user 4.32 ms, sys: 546 µs, total: 4.87 ms\n",
      "Wall time: 2.88 s\n",
      "6\n",
      "CPU times: user 7.69 ms, sys: 973 µs, total: 8.67 ms\n",
      "Wall time: 2.94 s\n"
     ]
    }
   ],
   "source": [
    "#let's vary the number of processes\n",
    "for NP in [1,2,4,8]:\n",
    "    print(NP)\n",
    "    with mp.Pool(NP) as pool :\n",
    "        %time  pool.map(task, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we increase the number of processes, the overhead increases and after some value this actually hurts the overall performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> \n",
    "\n",
    "So, as we just saw, multiprocessing works better when the individual tasks are longer. \n",
    "\n",
    "## Exercise: \n",
    "\n",
    " * re-think the `integrate_f_native` function so it is parallelizable in a few large tasks (rather than a lot of small tasks as we have done before) ?\n",
    " * implement your chosen solution.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solutions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "concept:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r -3 solutions/03_multiprocess_integrate.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "function definitions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 4-35 solutions/03_multiprocess_integrate.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "application:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 36- solutions/03_multiprocess_integrate.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "[back to the toc](#toc)\n",
    "\n",
    "\n",
    "## 2. numba and parallelization  <a id='9'></a>\n",
    "\n",
    "It is possible to provide a `numba` function to `mp.pool`, but `numba` already provides what's necessary to parallelize your code.\n",
    "\n",
    "By setting `parallel=True` when callin `@jit` (in no-python mode), numba will attempt to automatically parallelize your code.\n",
    "\n",
    "In particular, by default, it works on the array operations.\n",
    "\n",
    "### 2.1 automatic parallelization <a id='2.1'></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "native         : 0.6468\n",
      "numba          : 0.6467999999999999\n",
      "numba parallel : 0.6468\n"
     ]
    }
   ],
   "source": [
    "from numba import njit\n",
    "\n",
    "def integrate_f(a, b, N):\n",
    "    dx = (b - a) / N\n",
    "    X = np.arange(a,b,dx)\n",
    "    return ( X**2 - X ).sum() * dx\n",
    "\n",
    "## njit is a shortcut for jit(nopython=True)\n",
    "## the code does not change, so no need to re-write it; just give the function to njit\n",
    "integrate_f_numba = njit(integrate_f)\n",
    "\n",
    "integrate_f_numba_parallel = njit(integrate_f , parallel=True)\n",
    "\n",
    "## check that we get similar results: (+, this let's numba do the compilation now)\n",
    "print( \"native         :\", integrate_f(0,2,100) )\n",
    "print( \"numba          :\", integrate_f_numba(0,2,100) )\n",
    "print( \"numba parallel :\", integrate_f_numba_parallel(0,2,100) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "native         :\n",
      "48.6 ms ± 1.8 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "numba          :\n",
      "70.9 ms ± 439 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "numba parallel :\n",
      "5.85 ms ± 157 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "# now let's time it\n",
    "N = 10**7\n",
    "print( \"native         :\")\n",
    "%timeit integrate_f(0,2,N) \n",
    "print( \"numba          :\")\n",
    "%timeit integrate_f_numba(0,2,N)\n",
    "print( \"numba parallel :\")\n",
    "%timeit integrate_f_numba_parallel(0,2,N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the basic numba seems less efficient than numpy, but the parallel version is showing quite a speedup!\n",
    "\n",
    "Here, numba was able to parallelize the `np.arange`, all the array operations, and the `sum()`, so actually almost all the code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[back to the ToC](#toc)\n",
    "\n",
    "### 2.2 explicit parallelization (prange) <a id='2.2'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "native         : 0.6467999999999999\n",
      "numba          : 0.6467999999999999\n",
      "numba parallel : 0.6467999999999999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wandrille/Installed_software/anaconda3/envs/py38/lib/python3.8/site-packages/numba/core/typed_passes.py:329: NumbaPerformanceWarning: \u001b[1m\n",
      "The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible.\n",
      "\n",
      "To find out why, try turning on parallel diagnostics, see https://numba.readthedocs.io/en/stable/user/parallel.html#diagnostics for help.\n",
      "\u001b[1m\n",
      "File \"../../../../../../../tmp/ipykernel_4256/1462592780.py\", line 1:\u001b[0m\n",
      "\u001b[1m<source missing, REPL/exec in use?>\u001b[0m\n",
      "\u001b[0m\n",
      "  warnings.warn(errors.NumbaPerformanceWarning(msg,\n"
     ]
    }
   ],
   "source": [
    "def integrate_f2(a, b, N):\n",
    "    dx = (b - a) / N\n",
    "    s =0\n",
    "    for i in range(N):\n",
    "        x = a+i*dx\n",
    "        s += x**2-x\n",
    "    return s * dx\n",
    "\n",
    "integrate_f2_numba = njit(integrate_f2)\n",
    "\n",
    "integrate_f2_numba_parallel = njit(integrate_f2,parallel=True)\n",
    "\n",
    "\n",
    "## check that we get similar results: (+, this let's numba do the compilation now)\n",
    "print( \"native         :\", integrate_f2(0,2,100) )\n",
    "print( \"numba          :\", integrate_f2_numba(0,2,100) )\n",
    "print( \"numba parallel :\", integrate_f2_numba_parallel(0,2,100) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6468"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numba import prange \n",
    "\n",
    "@njit(parallel=True)\n",
    "def integrate_f2_numba_parallel(a, b, N):\n",
    "    dx = (b - a) / N\n",
    "    s =0\n",
    "    for i in prange(N):\n",
    "        x = a+i*dx\n",
    "        s += x**2-x\n",
    "    return s * dx\n",
    "integrate_f2_numba_parallel(0,2,100) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numba from native         :\n",
      "10 ms ± 877 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "numba parallel with prange:\n",
      "6.56 ms ± 895 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "# now let's time it\n",
    "N = 10**7\n",
    "print( \"numba from native         :\")\n",
    "%timeit -n 10 -r 7 integrate_f2_numba(0,2,N)\n",
    "print( \"numba parallel with prange:\")\n",
    "%timeit -n 10 -r 7 integrate_f2_numba_parallel(0,2,N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use a bit more data to compare the 2 parallel versions (auto and manual):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numba parallel auto:\n",
      "54.5 ms ± 5.4 ms per loop (mean ± std. dev. of 7 runs, 3 loops each)\n",
      "numba parallel with prange:\n",
      "52.9 ms ± 4.32 ms per loop (mean ± std. dev. of 7 runs, 3 loops each)\n"
     ]
    }
   ],
   "source": [
    "N = 10**8\n",
    "print( \"numba parallel auto:\")\n",
    "%timeit -n 3 -r 7 integrate_f_numba_parallel(0,2,N)\n",
    "print( \"numba parallel with prange:\")\n",
    "%timeit -n 3 -r 7 integrate_f2_numba_parallel(0,2,N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two results are similar. The one you end up using will depend on the structure of your problem and the shape of your code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "[back to ToC](#toc)\n",
    "\n",
    "### 2.3 controling the number of threads used <a id=\"2.3\"></a>\n",
    "\n",
    "Up until now, we have let numba use its default number of threads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numba\n",
    "numba.config.NUMBA_DEFAULT_NUM_THREADS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To control the number of threads, just use the `set_num_threads` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "default number of threads\n",
      "55.1 ms ± 5.73 ms per loop (mean ± std. dev. of 7 runs, 3 loops each)\n",
      "2\n",
      "65 ms ± 16 ms per loop (mean ± std. dev. of 7 runs, 3 loops each)\n",
      "3\n",
      "61.2 ms ± 7.26 ms per loop (mean ± std. dev. of 7 runs, 3 loops each)\n",
      "4\n",
      "50.9 ms ± 1.66 ms per loop (mean ± std. dev. of 7 runs, 3 loops each)\n",
      "5\n",
      "62.9 ms ± 3.85 ms per loop (mean ± std. dev. of 7 runs, 3 loops each)\n",
      "6\n",
      "55.3 ms ± 3.39 ms per loop (mean ± std. dev. of 7 runs, 3 loops each)\n",
      "7\n",
      "50.8 ms ± 1.38 ms per loop (mean ± std. dev. of 7 runs, 3 loops each)\n"
     ]
    }
   ],
   "source": [
    "from numba import set_num_threads\n",
    "\n",
    "N = 10**8\n",
    "\n",
    "## max number of threads\n",
    "print(\"default number of threads\")\n",
    "%timeit -n 3 -r 7 integrate_f2_numba_parallel(0,2,N)\n",
    "\n",
    "for num_thread in range(2,8):\n",
    "    print(num_thread)\n",
    "    set_num_threads(num_thread)\n",
    "    %timeit -n 3 -r 7 integrate_f2_numba_parallel(0,2,N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To go further, we recommend you have a look at [numba documentation on parallelization](https://numba.pydata.org/numba-doc/latest/user/parallel.html) which explains what can, and what cannot be parallelized, and how to diagnose the automatic parallelization process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "\n",
    "[back to the ToC](#toc)\n",
    "\n",
    "# Annex A - parallelization of pairwise distance computation with multiprocess <a id='annexa'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "def pairwise_distance_numpy(X):\n",
    "\n",
    "    num_vectors = X.shape[0]\n",
    "    num_measurements = X.shape[1] \n",
    "    D = np.empty((num_vectors, num_vectors), dtype=np.float64)\n",
    "    \n",
    "    for i in range(num_vectors):\n",
    "        for j in range(num_vectors):\n",
    "            d = np.square( np.subtract(X[i], X[j]) )\n",
    "            D[i, j] = np.sqrt(np.sum(d))\n",
    "    return(D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right now, this function operates onto a whole array.\n",
    "\n",
    "Ideally, we would like to reduce this something that looks like:\n",
    "\n",
    "```python\n",
    "\n",
    "for i in range(len(data)):\n",
    "    result[i] = function(data[i])\n",
    "\n",
    "```\n",
    "equivalent to:\n",
    "```python\n",
    "map(function,data)\n",
    "```\n",
    "\n",
    "So, we apply a `function` to each element (`data[i]`) of `data`.\n",
    "\n",
    "**Question:** how can we go from the `pairwise_distance_numpy` function to this? what would be `function`? `data`? \n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ... don't scroll - spoilers ahead ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "So, my proposition to solve this (not the only one possible, maybe not even the best) is that :\n",
    " 1. the `function` is computing distance between 2 vectors\n",
    " 2. the `data[i]` is a couple of vector\n",
    " 3. consequently, `data` is a list of couples of vectors.\n",
    "\n",
    "\n",
    "I will even go one (small) step further, and rather than keeping the whole vectors in data, I will just keep the vector indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "## generate 200 vectors with 100 measurements each \n",
    "data = np.random.uniform(size=(200,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "238 ms ± 3.51 ms per loop (mean ± std. dev. of 3 runs, 1 loop each)\n",
      "237 ms ± 17.3 ms per loop (mean ± std. dev. of 3 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "def pairwise_list_I(X):\n",
    "    \"\"\" create a list of the pairs of vector index we have to compute distances for (ie. all possible pair of indexes)\"\"\"\n",
    "    list_of_tuples = list()\n",
    "    \n",
    "    num_vectors = X.shape[0]\n",
    "    num_measurements = X.shape[1] \n",
    "    \n",
    "    for i in range(num_vectors):\n",
    "        for j in range(num_vectors):\n",
    "            list_of_tuples.append((i,j))\n",
    "            \n",
    "    return list_of_tuples\n",
    "\n",
    "def pairwise_distance_from_indexes(indexes ):\n",
    "    \"\"\"takes a tuple containing a pair of  indexes, and computes the distance between the 2\"\"\"\n",
    "    assert(len(indexes) == 2)\n",
    "    X1 = data[indexes[0]]\n",
    "    X2 = data[indexes[1]]\n",
    "    \n",
    "    return np.sqrt( np.sum( np.square( X1-X2 ) ) )\n",
    "\n",
    "\n",
    "list_of_tuples_I = pairwise_list_I(data)\n",
    "\n",
    "%timeit -n 1 -r 3  result = list(map(pairwise_distance_from_indexes,  list_of_tuples_I))\n",
    "\n",
    "import multiprocessing as mp\n",
    "\n",
    "with mp.Pool(2) as pool :\n",
    "    \n",
    "    %timeit -n 1 -r 3  result2 = pool.map(pairwise_distance_from_indexes, list_of_tuples_I)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some speedup, but nothing tremendous.\n",
    "\n",
    "Let's see if that holds up :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "CPU times: user 10.5 ms, sys: 11.8 ms, total: 22.3 ms\n",
      "Wall time: 352 ms\n",
      "2\n",
      "CPU times: user 26.8 ms, sys: 11.3 ms, total: 38.1 ms\n",
      "Wall time: 246 ms\n",
      "3\n",
      "CPU times: user 45 ms, sys: 0 ns, total: 45 ms\n",
      "Wall time: 197 ms\n",
      "4\n",
      "CPU times: user 39.6 ms, sys: 4.51 ms, total: 44.1 ms\n",
      "Wall time: 177 ms\n",
      "5\n",
      "CPU times: user 45.7 ms, sys: 4.82 ms, total: 50.6 ms\n",
      "Wall time: 199 ms\n",
      "6\n",
      "CPU times: user 57.6 ms, sys: 4.79 ms, total: 62.4 ms\n",
      "Wall time: 181 ms\n"
     ]
    }
   ],
   "source": [
    "for NP in [1,2,3,4,5,6]:\n",
    "    print(NP)\n",
    "    with mp.Pool(NP) as pool :\n",
    "        %time result = pool.map(pairwise_distance_from_indexes, list_of_tuples_I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "237 ms ± 13.1 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "## of course, we want to compare this with the original version of the function\n",
    "%timeit pairwise_distance_numpy( data )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So there is some gain. \n",
    "Nothing tremendous, but sill a 1.5x speefup, and it beats the alternative of having all these core idle.\n",
    "\n",
    "<br>\n",
    "\n",
    "Going one step further, we know multiprocessing works better when the task are somewhat large. So, instead of say that a task is \"compute a single distance\", and having NxN tasks, we could have the task be \"compute a full row of the distance matrix\", and then we only have N tasks.\n",
    "\n",
    "So for the task, we compute the distance between one vector and all the others. You will see this is a very good idea, even in a non-multiprocessing framework, because this plays into some of numpy's strength.\n",
    "\n",
    "First, let's implement our \"task\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 4.12395672, 4.00365722, 4.51227131, 4.02450461,\n",
       "       3.73729676, 3.81267711, 4.19146637, 3.83317922, 3.91421242])"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## computing reference results for testing validity\n",
    "toy_data = np.random.uniform(size=(10,100))\n",
    "\n",
    "res = pairwise_distance_numpy( toy_data )\n",
    "#we want our task to compute something like this : \n",
    "res[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         4.12395672 4.00365722 4.51227131 4.02450461 3.73729676\n",
      " 3.81267711 4.19146637 3.83317922 3.91421242]\n",
      "[ True  True  True  True  True  True  True  True  True  True]\n"
     ]
    }
   ],
   "source": [
    "## we have seen that numpy makes operation between 2 vector easy.\n",
    "## but actually, operation between a matrix and a vector works as well\n",
    "## so, matrix - vector will perform the subtraction on each row independently.\n",
    "## then, if we make the sum also on each row independently we can get the distances we want!\n",
    "\n",
    "def compute_distance_row( i ):\n",
    "    ## Here, I presume that there exists a DATA_GLOB\n",
    "    ##  variable in global memory with my data in it\n",
    "    \n",
    "    squared_diff = ( DATA_GLOB - DATA_GLOB[i])**2 ## squared differences between the matrix and a single vector \n",
    "    sums = np.sum( squared_diff , axis = 1) ## axis=1 --> to get 1 sum per row\n",
    "    return np.sqrt( sums ) ## compute square root of all these sums\n",
    "\n",
    "## of course we want to test this:\n",
    "DATA_GLOB = toy_data\n",
    "res_new = compute_distance_row( 0 )\n",
    "print(res_new)\n",
    "print(res_new == res[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All good so far. How does it perform?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.5 ms ± 2.87 ms per loop (mean ± std. dev. of 3 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "DATA_GLOB = data  # redefine the data used by the function as the bigger dataset\n",
    "\n",
    "## we map this onto the list of possible indices : from 0 to N\n",
    "%timeit -n 1 -r 3  result = list(map(compute_distance_row, range(DATA_GLOB.shape[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So you can see how this actually performs even better even with a single process.\n",
    "\n",
    "Actually, let's make the data larger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_data = np.random.uniform(size=(500,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40.9 ms ± 6.55 ms per loop (mean ± std. dev. of 3 runs, 3 loops each)\n",
      "1\n",
      "CPU times: user 5.87 ms, sys: 269 µs, total: 6.14 ms\n",
      "Wall time: 46 ms\n",
      "2\n",
      "CPU times: user 2.86 ms, sys: 3.86 ms, total: 6.71 ms\n",
      "Wall time: 28.5 ms\n",
      "3\n",
      "CPU times: user 0 ns, sys: 7.35 ms, total: 7.35 ms\n",
      "Wall time: 22.5 ms\n",
      "4\n",
      "CPU times: user 0 ns, sys: 9.04 ms, total: 9.04 ms\n",
      "Wall time: 19.7 ms\n",
      "5\n",
      "CPU times: user 13 ms, sys: 722 µs, total: 13.7 ms\n",
      "Wall time: 23.9 ms\n",
      "6\n",
      "CPU times: user 4.7 ms, sys: 11.6 ms, total: 16.3 ms\n",
      "Wall time: 28.1 ms\n"
     ]
    }
   ],
   "source": [
    "DATA_GLOB=big_data\n",
    "%timeit -n 3 -r 3  result = list(map(compute_distance_row, range(DATA_GLOB.shape[0])))\n",
    "for NP in [1,2,3,4,5,6]:\n",
    "    print(NP)\n",
    "    with mp.Pool(NP) as pool :\n",
    "        %time result = pool.map(compute_distance_row, range(DATA_GLOB.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as before: with larger individual takes we seem to get better speedup in general (~x2 speedup for 4processes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[back to the ToC](#toc)\n",
    "\n",
    "# Annex B - parallelization of pairwise distance computation with numba <a id='annexb'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import njit, prange\n",
    "\n",
    "# njit -> no-python jit\n",
    "\n",
    "@njit(parallel=True)\n",
    "def pairwise_distance_numba_prange(X):\n",
    "\n",
    "    num_vectors = X.shape[0]\n",
    "    num_measurements = X.shape[1] \n",
    "    D = np.empty((num_vectors, num_vectors), dtype=np.float64)\n",
    "    \n",
    "    for i in prange(num_vectors): # note usage of prange\n",
    "        for j in range(num_vectors):\n",
    "            d = 0.\n",
    "            for k in range(num_measurements):\n",
    "                d += np.square( np.subtract(X[i][k], X[j][k])  )\n",
    "            D[i, j] = np.sqrt(d)\n",
    "    return(D)\n",
    "\n",
    "toydata = np.random.uniform(size=(10,10)) # I make toy data to launch the function once and compile it\n",
    "toyresult = pairwise_distance_numba_prange( toydata ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numba parallel=True\n",
      "22.6 ms ± 5.43 ms per loop (mean ± std. dev. of 3 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "print(\"numba parallel=True\")\n",
    "%timeit -n 1 -r 3 result = pairwise_distance_numba_prange( big_data )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_py38)",
   "language": "python",
   "name": "conda_py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
