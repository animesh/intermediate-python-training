{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 2 - Statistics with python\n",
    "---------------------------------------------------\n",
    "\n",
    "This notebook aims to present how to perform classical statiscial procedure as well as some amount of regression using various python libraries, such as `scipy`.\n",
    "\n",
    "It **does not aim to replace a course on statistics**, but rather focuses on the code aspect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toc <a id='toc'></a>\n",
    "\n",
    "<br>\n",
    "\n",
    " 1. [SciPy.stats and statistics in python](#stats)\n",
    "\n",
    "    1.1 [manipulation of random distributions](#stats.1)\n",
    "\n",
    "    1.1.1 [Drawing some random numbers : rvs](#stats.1.1)\n",
    "\n",
    "    1.1.2 [Looking up the quantiles and probability density functions](#stats.1.2)\n",
    "\n",
    "    1.2 [t-test](#stats.2)\n",
    "\n",
    "    1.3 [statistical power calculations](#stats.3)\n",
    "\n",
    "    1.4 [Multiple hypothesis testing](#stats.4)\n",
    "\n",
    "    1.5 [Fisher's exact test and the Chi-square test](#stats.5)\n",
    "\n",
    "    1.6 [1-way anova](#stats.6)\n",
    "<br>\n",
    "\n",
    " 2. [Correlation and linear regression](#reg)\n",
    "\n",
    "    2.1 [Correlation](#reg.1)\n",
    "\n",
    "    2.2 [Regression](#reg.2)\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 1. `scipy.stats` and statistics in python <a id='stats'></a>\n",
    "\n",
    "\n",
    "**[SciPy](https://scipy.org)** is a comprehensive project for scientific python programming, regrouping a [library](https://docs.scipy.org/doc/scipy/reference/) and implementing various tools and algorithm for scientific software.\n",
    "\n",
    "This section gives a primer on the **`scipy.stats`** library, which provides ways to interact with various random distribution functions, and implements numerous statistical tests.\n",
    "\n",
    "\n",
    "\n",
    "### 1.1 manipulation of random distributions <a id='stats.1'></a>\n",
    "\n",
    "The **`scipy.stats`** module implements utilities for a large number of continuous and discrete distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "dist_continu = [d for d in dir(stats) if isinstance(getattr(stats, d), stats.rv_continuous)]\n",
    "dist_discrete = [d for d in dir(stats) if isinstance(getattr(stats, d), stats.rv_discrete)]\n",
    "print('number of continuous distributions: %d' % len(dist_continu))\n",
    "print('number of discrete distributions:   %d' % len(dist_discrete))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's experiment with the normal distribution, or `norm` in `scipy.stats`\n",
    "\n",
    "A look at `help(stats.norm)` tells us that:\n",
    "* The **`loc`** argument (location) specifies the **mean** of the distribution.\n",
    "* The **`scale`** argument specifies the **standard deviation** of the distribution.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Example:** generate a specific normal distribution with a mean of 10 and a stdev of 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = stats.norm(loc=10, scale=2)\n",
    "\n",
    "# The mean and variance of a distribution can be retrieved using the .stats method :\n",
    "print(\"Type of N is:\", type(N))\n",
    "print(\"Mean and standard deviation:\", N.stats())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scipy distribution object (`N` in our example) can then be used to interact with the distribution in many ways, as illustrated below.\n",
    "\n",
    "### 1.1.1. Drawing random numbers: `rvs` <a id='stats.1.1'></a>\n",
    "\n",
    "The **`rvs()`** method allows to draw a random number of values from a distribution.\n",
    "* The `size` argument is 1 or several integers and defines the dimensions of the returned arrays of\n",
    "  random numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N.rvs(size=[50]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(N.rvs(size=[5000])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Warning:** as with any drawing of random variable on a computer, [one merely emulates randomness](https://en.wikipedia.org/wiki/Pseudorandom_number_generator).  \n",
    "\n",
    "The positive aspect of using pseudo-random numbers is that one can make random operation reproducible by setting up **random seed**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Set the random seed and draw 5 random numbers:\n",
    "np.random.seed(2021)   \n",
    "draw1 = N.rvs(size=5)\n",
    "\n",
    "# Set the random seed back to the same value as above -> 2021.\n",
    "np.random.seed(2021)\n",
    "draw2 = N.rvs(size=5)\n",
    "\n",
    "print(\"Are the ramdom draws equal?\", all(draw1 == draw2))\n",
    "print(draw1)\n",
    "print(draw2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 1.1.2 Looking-up quantiles and probability density functions <a id='stats.1.2'></a>\n",
    "\n",
    "* **`pdf()`**: Probability Density Function.\n",
    "* **`cdf()`**: Cumulative Distribution Function.\n",
    "* **`ppf()`**: Percent Point Function (inverse of CDF), gives the quantiles of the distribution.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Example:** using the PDF to plot a distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "X = np.arange(0, 20, 0.1)      # generate a sequence ranging from 0 to 20 with increment of 0.1\n",
    "plt.plot(X, N.pdf(X))          # for each element of the sequence, get the associated PDF value.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**Example:** using the CDF to estimate a probability, using PPF to compute quantiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cdf: Cumulative Distribution Function.\n",
    "print('What is the probability of drawing a number <= 15.0 ?', N.cdf(15.0))\n",
    "\n",
    "# ppf: gives the quantiles of the distribution.\n",
    "P = [0.025, 0.5, 0.975]\n",
    "Q = N.ppf(P)\n",
    "print('Quantiles:', P, '->', Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "For **discrete distribution** these rules change a bit: `pdf()` is replaced by `pmf()`.\n",
    "\n",
    "Let's see an example of **binomial distribution** with 10 draws and a 0.5 probability of success:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.arange(0,10)\n",
    "plt.scatter(X, stats.binom.pmf(X, n=10, p=0.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most distributions have a certain number of **parameters** which may control their overall **shape, location or scale**.\n",
    "* The **normal law** e.g. has two parameters: its mean ($\\mu$) and its standard deviation ($\\sigma$), \n",
    "  which respectively control its location and scale. In `scipy`, these are set via the `loc` (location)\n",
    "  and `scale` arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = np.linspace(-10,10, 100) # 100 equally separated points between -5 and 5\n",
    "fig, ax = plt.subplots(figsize=(14, 7))\n",
    "\n",
    "a= sns.lineplot(x=x, y=stats.norm.pdf(x , loc = 0 , scale = 1) , ax = ax , label='mean= 0 , stdev=1')\n",
    "sns.lineplot(x=x, y=stats.norm.pdf(x , loc = -2 , scale = 1 ) , ax = ax , label='mean=-2 , stdev=1')\n",
    "sns.lineplot(x=x, y=stats.norm.pdf(x , loc = 1 , scale = 3 ) , ax = ax , label='mean= 1 , stdev=3')\n",
    "\n",
    "a.set(xlabel='value', ylabel='density')\n",
    "a.set_title('normal law - probability density function')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[back to toc](#toc)\n",
    "\n",
    "<br>\n",
    "\n",
    "## 1.2 t-test <a id='stats.2'></a>\n",
    "\n",
    "The **t-test** is used to determine if the means of two samples (drawn from 2 sub-populations for instance) are significantly different.\n",
    "\n",
    "It is a widely used test, with important but not overly complex assumptions:\n",
    " * Independence of data points.\n",
    " * The means of each sample should follow normal distributions.\n",
    " * *The two sample share the came variance* <- there are different flavors of the t-test depending\n",
    "   on that assumption.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Example:** weight of mices of different genotypes and subjected to different diets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mice_data = pd.read_csv('data/mice_data.csv')\n",
    "sns.catplot(x='weight' , y='genotype' , data=mice_data , kind='violin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WTdata = mice_data['weight'][ mice_data['genotype'] == 'WT' ]\n",
    "KOdata = mice_data['weight'][ mice_data['genotype'] == 'KO' ]\n",
    "\n",
    "tstat , pval = stats.ttest_ind( KOdata , WTdata , equal_var=False)#use equal_var=True if you have tested for variance equality\n",
    "print('test statistic: ',tstat , sep='\\t')\n",
    "print('p-value : ',pval , sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Micro-exercise:\n",
    "* There is another column in the data-set : `diet`, which can take the values `\"HFD\"` and `\"CHOW\"`.\n",
    "  Perform a t-test exactly as before, but splitting mice by their `diet` rather than their `genotype`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mice_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[back to toc](#toc)\n",
    "\n",
    "<br>\n",
    "\n",
    "## 1.3 Statistical power calculations <a id='stats.3'></a>\n",
    "\n",
    "\n",
    "For wome widely used tests, functions let you automatically compute statistical power for a given effect size or sample size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.power import TTestIndPower\n",
    "\n",
    "\n",
    "mean_difference=1\n",
    "standard_dev=1\n",
    "sample_size=10\n",
    "\n",
    "effect_size = mean_difference/standard_dev\n",
    "\n",
    "P = TTestIndPower()\n",
    "print( 'power:' , P.power(effect_size=effect_size , \n",
    "                          nobs1=sample_size , \n",
    "                          ratio=1 , alpha=0.05) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculating statistical power can help inform our experimental design**. \n",
    "\n",
    "For example, how many observation per sample do we need if we want to detect a difference in mean of 1 with significance level (type I error) 0.01 and statistical power 0.80:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "effect_size=1\n",
    "sig_threshold = 0.01\n",
    "P = TTestIndPower()\n",
    "\n",
    "powers = []\n",
    "for sample_size in range(2,50):\n",
    "    powers.append( P.power(effect_size=effect_size , nobs1=sample_size , ratio=1 , alpha=sig_threshold) )\n",
    "\n",
    "fig,ax=plt.subplots(figsize=(14,7))\n",
    "sns.lineplot(range(2,50) , powers , label = 'effect size='+str(effect_size) , ax=ax)\n",
    "ax.axhline(0.8, color='r', linestyle='-')\n",
    "ax.set(xlabel='sample size', ylabel='power')\n",
    "\n",
    "\n",
    "## or, directly:\n",
    "print( 'minimum sample size:', P.solve_power(effect_size=effect_size , \n",
    "                                             nobs1=None ,  \n",
    "                                             ratio=1 , \n",
    "                                             alpha=sig_threshold , \n",
    "                                             power = 0.8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[back to toc](#toc)\n",
    "\n",
    "<br>\n",
    "\n",
    "## 1.4 Multiple hypothesis testing <a id='stats.4'></a>\n",
    "\n",
    "\n",
    "Recall the definition of the p-value: the probability of obtaining a test statistic at least as extreme as the one observed, **if the null hypothesis is true**\n",
    "\n",
    "Thus, *even* if the p-value is, let's say, 0.04, there is still a 4% chance of obtaining such an extreme result by chance.\n",
    "\n",
    "This is often acceptable if we only perform one test, if we perform many tests we have seen (with the simulations), that even when there is no real effect some tests will turn out significant by chance.\n",
    "\n",
    "> This is the definition of the $\\alpha$ risk fo type I error is.\n",
    "\n",
    "Of course, this has important implication for science and the relevance of our results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](img/xkcd882.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> source : [xkcd](http://xkcd.com) (note: there are many relevant xkcd strips for everything related to stats/programming courses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = np.array([1, 2, 5, 10, 50, 100])\n",
    "fig,ax=plt.subplots(figsize=(14,7))\n",
    "sns.lineplot( P , 1- 0.95**P ,ax=ax)\n",
    "ax.set(xlabel='number of tests', ylabel='probability of at least 1 test significant by chance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "We need to change perspective.\n",
    "Instead of trying to limit the false positive probability for *each* test, we focus on:\n",
    "* the probability of obtaining **any** false positives (family-wise error rate, **FWER**)\n",
    "* the proportion of false positives among all findings (false discovery rate, **FDR**)\n",
    "\n",
    "> Controlling the FWER is often too stringent - limit type I errors, but get lots of type II errors. \n",
    "\n",
    "### The Bonferroni method for controlling the FWER\n",
    "\n",
    "- Assume we are performing $N$ tests\n",
    "- To control the FWER at (e.g.) 0.05, only call variables with p-values below $0.05/N$ significant\n",
    "\n",
    "\n",
    "\n",
    "### The Benjamini-Hochberg method for controlling the FDR\n",
    "\n",
    "- Assume we are performing $N$ tests\n",
    "- Intuition: for each p-value threshold $\\alpha$, we can estimate the number of false discoveries by $\\alpha N$\n",
    "- Compare this to the actual number of discoveries at the threshold - $N_\\alpha$\n",
    "- Choose a p-value threshold $\\alpha$ such that $\\alpha N/N_\\alpha$ is less than a desired threshold (e.g. 0.05) - this threshold would give an expected FDR of 0.05\n",
    "- Note that the FDR is truly a property of a *set* - in a set of genes with FDR = 0.05, we can expect around 5% to be false discoveries. However, we don't know *which* ones! It could be the most significant!\n",
    "- Often, we want a gene-wise measure of significance (like the p-value)\n",
    "- The q-value, or adjusted p-value, of a variable is the *smallest* FDR we have to accept in order to call that variable significant.\n",
    "- For example, if the adjusted p-value is 0.2, we have to accept that if we want to call this variable (and consequently, all variables with lower p-values) significant, there will be approximately 20% false discoveries among them.\n",
    "\n",
    "In python :\n",
    "\n",
    "```python\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "# Bonferroni\n",
    "rejected,fwers,alphacSidak,alphacBonf = multipletests(pvals, alpha=0.05, method='bonferroni')\n",
    "\n",
    "# BH procedure\n",
    "rejected,fdrs,alphacSidak,alphacBonf = multipletests(pvals, alpha=0.05, method='fdr_bh')\n",
    "\n",
    "# Note: many other methods are available -> help(multipletests)\n",
    "```\n",
    "\n",
    "* rejected : true for hypothesis that can be rejected for the given alpha\n",
    "* fwers|fdrs : p-values corrected for multiple tests\n",
    "* alphacSidak : corrected alpha for Sidak method\n",
    "* alphacBonf : corrected alpha for Bonferroni method\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imagine we perform 10 000 tests of 10 000 random data-sets\n",
    "pvals = []\n",
    "\n",
    "N=10000\n",
    "mean_difference=0 # no differences -> any detected difference is due to chance\n",
    "sample_size = 100\n",
    "std=1\n",
    "\n",
    "for i in range(N):\n",
    "    t , pval_ttest = stats.ttest_ind( np.random.randn( sample_size ) * std , \n",
    "                                     np.random.randn( sample_size ) * std + mean_difference ,equal_var=True)\n",
    "    \n",
    "    pvals.append(pval_ttest)\n",
    "pvals = np.array(pvals)\n",
    "# stats models proposes a function implementing numerous p-value correction methods\n",
    "# https://www.statsmodels.org/dev/generated/statsmodels.stats.multitest.multipletests.html\n",
    "\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "rejected,fwers,alphacSidak,alphacBonf = multipletests(pvals, alpha=0.05, method='bonferroni')\n",
    "rejected,fdrs,alphacSidak,alphacBonf = multipletests(pvals, alpha=0.05, method='fdr_bh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(figsize=(14,7))\n",
    "sns.histplot( pvals ,bins = np.arange(0,1.05,0.01) ,  ax = ax , label = 'p-value' , color='blue')\n",
    "sns.histplot( fwers ,bins = np.arange(0,1.05,0.01) ,   ax = ax , label = 'FWER' , color='yellow')\n",
    "sns.histplot( fdrs  ,bins = np.arange(0,1.05,0.01) ,  ax = ax , label = 'FDR' , color='green')\n",
    "\n",
    "print('Fraction of (spuriously) significant tests:')\n",
    "print('p-value:' , sum(pvals<0.05)/N )\n",
    "print('FWER   :' , sum(fwers<0.05)/N )\n",
    "print('FDR    :' , sum(fdrs <0.05)/N )\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[back to toc](#toc)\n",
    "\n",
    "<br>\n",
    "\n",
    "## 1.5 Fisher's exact test and the Chi-square test <a id='stats.5'></a>\n",
    "\n",
    "These two tests have for object the association between 2 categorical variables.\n",
    "\n",
    "Their **null hypothesis** is the absence of association between the two variable.\n",
    "\n",
    "\n",
    "**Fisher's exact test**, as its name entails, computes a p-value which is exact, even for very low smaple sizes. However it becomes computationnaly complex to compute as the data set size or number of categories gets high.\n",
    "\n",
    "The **Chi-square test**, in contrast, uses an approximation of the exact p-value which is only valid when samples are big enough. However, it scales well to larger samples sizes and number of categories.\n",
    "\n",
    "\n",
    "Both tests start from a **contingency table**.\n",
    "\n",
    "We are going to use as example the historical [Lady tasting tea](https://en.wikipedia.org/wiki/Lady_tasting_tea).\n",
    "\n",
    "|  | detected as milk before | detected as milk after | marginal sums |\n",
    "|---|---|---|---|\n",
    "| **milk before** | 3 | 1 | **4** |\n",
    "| **milk after** | 1 | 3 | **4** |\n",
    "| **marginal sums**  | **4** | **4** | **8** |\n",
    "\n",
    "In our experiment, the lady was able to correctly identify 6 out of 8 cups.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## fisher exact test\n",
    "\n",
    "table = [[3,1],[1,3]]\n",
    "\n",
    "oddsratio , pvalue = stats.fisher_exact(table)\n",
    "print(\"Fisher's exact test\")\n",
    "print('\\todds ratio:',oddsratio)\n",
    "print('\\tp-value:',pvalue)\n",
    "\n",
    "### chi square\n",
    "chi2,pval , df, expected = stats.chi2_contingency(table , correction=False)\n",
    "print(\"Chi-square test\")\n",
    "print('\\tchi2:', chi2)\n",
    "print('\\tp-value:', pval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see here that the returned p-value is quite different from the one given by Fisher's exact test.\n",
    "\n",
    "> note that here we use `correction=False` as by default scipy implementation uses [Yates's correction](https://en.wikipedia.org/wiki/Yates%27s_correction_for_continuity), which is useful when the effectives are low. Try the same lines with the correction to see the difference.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's imagine now that we have a many cups and very patient lady so that the contigency table looks like this:\n",
    "\n",
    "|  | detected as milk before | detected as milk after | marginal sums |\n",
    "|---|---|---|---|\n",
    "| **milk before** | 25 | 15 | **40** |\n",
    "| **milk after** | 18 | 22 | **40** |\n",
    "| **marginal sums**  | **40** | **40** | **80** |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = [[25,15],[18,22]]\n",
    "\n",
    "oddsratio , pvalue = stats.fisher_exact(table)\n",
    "print(\"Fisher's exact test\")\n",
    "print('\\todds ratio:',oddsratio)\n",
    "print('\\tp-value:',pvalue)\n",
    "\n",
    "chi2,pval , df, expected = stats.chi2_contingency(table , correction=False)\n",
    "print(\"Chi-square test\")\n",
    "print('\\tchi2:', chi2)\n",
    "print('\\tp-value:', pval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the p-value of the Chi-square test is now much closer to that of Fisher's exact test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Excercise\n",
    "\n",
    "Come back to the census data from 1880, in particular in the file `'census1880_fractions.csv'` file we have kept the data as fraction and created a few categories.\n",
    "\n",
    "Test the association between majority religion (`'majority_religion'`) and majority language (`'majority_language'`).\n",
    "\n",
    "> Tip: to create a contingency table :\n",
    "\n",
    "> ```table = pd.crosstab( dfFractions['majority religion'] , dfFractions['majority language'] )```\n",
    "\n",
    "\n",
    "**extra question if you have the time :** How could you make Fisher's test work here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfFractions = pd.read_csv('data/census1880_fractions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfFractions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[back to toc](#toc)\n",
    "\n",
    "<br>\n",
    "\n",
    "## 1.6 1-way anova <a id='stats.6'></a>\n",
    "\n",
    "The ANOVA, or ANalyse Of VAriance, stands maybe among the most used (and abused) type of statistical tests to date.\n",
    "\n",
    "The anova is used to analyze the differences among group means in a sample. \n",
    "In particular, we are going to concentrate here on the 1-way ANOVA, which evaluates the difference in means of a numerical variable across groups formed by another (single) variable.\n",
    "\n",
    "In this sense, it is a generalization of the t-test which is limited to 2 groups only (in fact, the 1-way anova and t-test are quivalent when there are only 2 groups).\n",
    "\n",
    "**Anova assumptions** :\n",
    "* subpopulation distributions are normal\n",
    "* samples have equal variances\n",
    "* observations are independent from one another\n",
    "\n",
    "**Test hypothesis** : \n",
    "given $m$ groups of mean $\\bar{x}_{1...m}$, each containing $n_i$ observations (for a total of $n$)\n",
    " * **Null hypothesis** : $H_0 : \\bar{x}_1 = \\bar{x}_2 = ... = \\bar{x}_m$\n",
    " * **Alternative hypothesis** : At least one of these means differ from the others\n",
    " \n",
    "The anova relies on the idea that if the mean varies between the different group then the overall variance of all samples should be significantly greater than the variance within each group (hence the name).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imagine we measure the height of the same number of individuals individuals in 3 plant subspecies\n",
    "sampleSize = 10\n",
    "numberOfGroups = 3\n",
    "dfPlant = pd.DataFrame({'subSpecies' : [0]*sampleSize + [1]*sampleSize + [2]*sampleSize ,\n",
    "                        'plantSize' : np.concatenate( ( np.random.randn(sampleSize) + 15 , \n",
    "                                                       np.random.randn(sampleSize) + 16 , \n",
    "                                                       np.random.randn(sampleSize) + 17 )) } )\n",
    "fig,axes = plt.subplots(1,2 , figsize=(14,7),  sharey=True )\n",
    "sns.distplot(dfPlant['plantSize'] , rug=True, ax = axes[0] , vertical=True)\n",
    "sns.violinplot(x='subSpecies', y='plantSize' , data=dfPlant, kind = 'violin' , ax = axes[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfPlant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Fstat , pval = stats.f_oneway( dfPlant['plantSize'][dfPlant['subSpecies'] == 0],\n",
    "                dfPlant['plantSize'][dfPlant['subSpecies'] == 1],\n",
    "                dfPlant['plantSize'][dfPlant['subSpecies'] == 2] )\n",
    "print('automated 1-way anova / F-test:')\n",
    "print('F-stat :',Fstat)\n",
    "print('p-value:',pval)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[back to toc](#toc)\n",
    "\n",
    "<br>\n",
    "\n",
    "# 2. Correlation and linear regression <a id='reg'></a>\n",
    "\n",
    "## 2.1 Correlation <a id='reg.1'></a>\n",
    "\n",
    "**Correlation** is a measure of the amount of relatedness between two measured variables. It comes in several flavours :\n",
    " * Pearson's linear correlation coefficient : for linear relationship only : `stats.pearsonr(x,y)`\n",
    " * Spearman's rank correlation coefficient : accepts non linear, but \"monotonic only\" : `stats.spearmanr(x,y)`\n",
    " * Kendall's Tau : relies on an order relation only and less influenced by the scale as the others : `stats.kendalltau(x,y)`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma=1./5\n",
    "\n",
    "linear=[[u,(u)/100+sigma*np.random.randn()] for u in range(10,500)]\n",
    "monotonic=[[u,50*(0.8**(u/10))+sigma*np.random.randn()] for u in range(10,500)]\n",
    "\n",
    "non_monotonic=[[u,(u)**3+3*u**2+sigma*np.random.randn()] for u in np.arange(-1,1,1./250)]\n",
    "\n",
    "together=[linear,monotonic,non_monotonic]\n",
    "plt.subplots(133,figsize=(15,5))\n",
    "for i in range(3):\n",
    "    plt.subplot(1,3,i+1)\n",
    "    x=[u[0] for u in together[i]]\n",
    "    y=[u[1] for u in together[i]]\n",
    "    plt.scatter(x,y)\n",
    "    plt.title('Pearson: {0:.3f}, Spearman: {1:.3f}, Kendall: {2:.3f}'.format(\n",
    "                                    stats.pearsonr(x,y)[0],##just like that\n",
    "                                    stats.spearmanr(x,y)[0],\n",
    "                                    stats.kendalltau(x,y)[0]))\n",
    "plt.tight_layout()    \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[back to toc](#toc)\n",
    "\n",
    "<br>\n",
    "\n",
    "## 2.2 Regression <a id='reg.2'></a>\n",
    "\n",
    "Performing regression (linear or otherwise) is possible with `scipy`, but it is not the best.\n",
    "\n",
    "`statsmodels` offers much nicer options and statistical reports. Additionnally, this is a great opportunity to see together how to install a library. \n",
    "\n",
    "1. go to [https://www.statsmodels.org](https://www.statsmodels.org)\n",
    "2. click on the install page \n",
    "3. find the instruction for installation with Anaconda \n",
    "4. type the command in either a terminal (mac,linux) or in the anaconda-prompt (windows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels\n",
    "print('OK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_diabetes = pd.read_table( \"data/diabetes.csv\" , sep=',' )\n",
    "df_diabetes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pulling all the data together in the same dataFrame\n",
    "\n",
    "x = df_diabetes['bmi'] # covariable bmi\n",
    "y = df_diabetes['disease progression'] # response variable disease progression\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots( figsize = (10,10)) #setup graphical windows\n",
    "sns.scatterplot(x=x,y=y) # plot x versus y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.pearsonr(df_diabetes['bmi'],\n",
    "               df_diabetes['disease progression'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We'll be using the statsmodel package, which computes a lot of cool metrics for you\n",
    "import statsmodels\n",
    "import statsmodels.api as sm\n",
    "\n",
    "X = sm.add_constant(x)## adding a intercept to the model\n",
    "model = sm.OLS(y, X)  ## defining an Ordinary Least Square variable\n",
    "results = model.fit() ## fitting it\n",
    "\n",
    "print( results.summary() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## diagnostic plots :\n",
    "from statsmodels.graphics.gofplots import qqplot\n",
    "fig,axes = plt.subplots(1,2,figsize = (15,6))\n",
    "\n",
    "# plotting residuals (error of the model) vs. fitted values (prediction fo the model)\n",
    "# helps determine homoskedascticity and sphericity of errors  :\n",
    "# the points should show about the same spread all along the x axis, and be centered around 0.\n",
    "axes[0].scatter( results.fittedvalues , results.resid )\n",
    "axes[0].axhline(0.0 , color = 'grey')\n",
    "axes[0].set_xlabel('fitted values')\n",
    "axes[0].set_ylabel('residuals')\n",
    "\n",
    "# the QQplot (quantile-quantile plot) is  great plot to assess normality of the model's residual\n",
    "# basically points should stay close to the diagonal line if they follow something close to a normal distribution.\n",
    "qqplot(results.resid , ax = axes[1] , line='s') \n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Plotting the fit\n",
    "from statsmodels.sandbox.regression.predstd import wls_prediction_std\n",
    "\n",
    "# we obtain the predicted values for our model, as well as their 95% intervals\n",
    "prstd, iv_l, iv_u = wls_prediction_std(results) \n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "\n",
    "#ax.plot(x, y, 'o', label=\"data\")\n",
    "sns.scatterplot(x=x,y=y) # plot x versus y\n",
    "ax.plot(x, results.fittedvalues, 'r', label=\"OLS regression result\")\n",
    "ax.plot(x, iv_u, color='orange',linestyle='--' , label='95% fit interval')\n",
    "ax.plot(x, iv_l, color='orange',linestyle='--')\n",
    "ax.legend(loc='best',fontsize=10)\n",
    "#plt.yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Going further : using R-style formula in statsmodels\n",
    "\n",
    "If you have been doing some regression in R, you would have come across *formula*, where instead of different dataframes for dependent variable covariables, on gives a single `DataFrame` along with a formular of the type `y ~ x`.\n",
    "\n",
    "These offer a fairly simple and elegant way of specifying your model, with an efficient hamdling of intercepts, as well as interaction effects.\n",
    "\n",
    "Going into details on these formula is beyond the scope of this course, but we encourage you to go though this very well made tutorial: https://www.statsmodels.org/devel/example_formulas.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
